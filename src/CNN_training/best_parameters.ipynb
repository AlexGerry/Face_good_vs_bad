{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from keras import layers, regularizers, optimizers, losses\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocessing\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from keras.utils import set_random_seed, image_dataset_from_directory\n",
    "from training import prepare_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "DATASET_PATH = \"../../../dataset/\"\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (224, 224, 3, )\n",
    "\n",
    "# `PYTHONHASHSEED` environment variable\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# Python built-in random, numpy(+ scikit) and tensorflow seed\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "print(\"Loading train dataset...\")\n",
    "train_ds = image_dataset_from_directory(\n",
    "    directory=os.path.join(DATASET_PATH, \"train\"),\n",
    "    label_mode=\"categorical\",\n",
    "    validation_split=None,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=None,\n",
    "    seed=SEED\n",
    "    )\n",
    "print(\"Train dataset loaded!\")\n",
    "print(\"Labels in the dataset: \", train_ds.class_names)\n",
    "\n",
    "# Load the validation dataset\n",
    "print(\"Loading validation dataset...\")\n",
    "val_ds = image_dataset_from_directory(\n",
    "    directory=os.path.join(DATASET_PATH, \"valid\"),\n",
    "    label_mode=\"categorical\",\n",
    "    validation_split=None,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=None,\n",
    "    seed=SEED\n",
    "    )\n",
    "print(\"Validation dataset loaded!\")\n",
    "\n",
    "train_ds = train_ds.shuffle(256, seed=SEED, reshuffle_each_iteration=False).take(int(len(train_ds)*0.25))\n",
    "val_ds = val_ds.shuffle(128, seed=SEED, reshuffle_each_iteration=False).take(int(len(val_ds)*0.25))\n",
    "\n",
    "train_ds = prepare_dataset(train_ds, augment=True)\n",
    "val_ds = prepare_dataset(val_ds)\n",
    "\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.Input(shape=(INPUT_SHAPE)))\n",
    "    \n",
    "    # Search first conv\n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Choice('conv_1_filter', values=[32, 64]),\n",
    "        kernel_size=hp.Choice('conv_1_kernel', values = [3,5]),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(1e-2),\n",
    "        strides=(1, 1)\n",
    "    ))\n",
    "    model.add(layers.MaxPooling2D(pool_size=hp.Choice('pool_1_size', values = [3,5])))\n",
    "    \n",
    "    # Choose how many conv layers\n",
    "    for i in range(hp.Int(\"num_Convolutional_layers\", 1, 2)):\n",
    "        model.add(\n",
    "            layers.Conv2D(\n",
    "                filters=hp.Choice(f\"conv_{i}_filters\", values=[64, 128, 256]),\n",
    "                kernel_size=(3, 3),\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=regularizers.l2(1e-2),\n",
    "                strides=(1, 1)\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # Choose how many classifier\n",
    "    for i in range(hp.Int(\"num_FullyConnected_layers\", 1, 2)):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                # Tune number of units separately.\n",
    "                units=hp.Choice(f\"units_{i}\", values=[64, 128, 256]),\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=regularizers.l2(1e-2)\n",
    "            )\n",
    "        )\n",
    "        if hp.Boolean(\"dropout\"): model.add(layers.Dropout(rate=0.25))\n",
    "    \n",
    "    model.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    # Choose the optimizer\n",
    "    hp_optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop', 'adamax'], default = 'adamax')\n",
    "    optimizer = tf.keras.optimizers.get(hp_optimizer)\n",
    "    # Choose the learning rate\n",
    "    optimizer.learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-2], default = 1e-3)\n",
    "                                        \n",
    "    model.compile(optimizer=optimizer, \n",
    "                    loss=\"categorical_crossentropy\", \n",
    "                    metrics = [\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./keras_tuner/\", exist_ok=True)\n",
    "tuner = kt.BayesianOptimization(build_model,\n",
    "                                objective=kt.Objective('val_loss', direction=\"min\"),\n",
    "                                directory='./keras_tuner',\n",
    "                                max_trials = 20, overwrite=False,\n",
    "                                project_name='tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combination of all parameters\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=12, min_delta=0.0001, restore_best_weights=True, verbose=1)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.001, patience=5, verbose=1)\n",
    "callbacks = [early_stopping, lr_scheduler]\n",
    "# Search best hyperparameter\n",
    "tuner.search(train_ds, epochs=150, validation_data=val_ds, shuffle=True, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top model\n",
    "models = tuner.get_best_models(num_models=1)\n",
    "best_model = models[0]\n",
    "# Build the model.\n",
    "# best_model.build(input_shape=INPUT_SHAPE)\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(\"./keras_tuner/\" + 'best_hyperparameter_tuned_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb528fd416c0ff975b48fb7dd275c91a0873308d73117ccfdcc096d65948d1b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
